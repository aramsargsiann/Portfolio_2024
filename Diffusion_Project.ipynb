{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aramsargsiann/Portfolio_2024/blob/main/Diffusion_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8usGWesCOCbK"
      },
      "source": [
        "**new code era**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz5XuUnaRh-e"
      },
      "source": [
        "**1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "brvAB66-k6S2",
        "outputId": "8ce27e52-116b-4b62-cd06-95945610b57f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-44a39d98-d0e2-49a1-9ec0-b39fc1b21441\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-44a39d98-d0e2-49a1-9ec0-b39fc1b21441\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving diffusion_model_classes.py to diffusion_model_classes.py\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'diffusion_model_classes.py': b'# -*- coding: utf-8 -*-\\n\"\"\"Diffusion_Model_Classes.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1VFUPUchJkf5SjoPpBqpWw9Mh2M_0Ogxi\\n\"\"\"\\n\\n# lybrires\\n\\nfrom tqdm import tqdm\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom IPython.display import HTML\\nfrom typing import Dict, Tuple\\nfrom torchvision import models, transforms\\nfrom torchvision.utils import save_image, make_grid\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.animation import FuncAnimation, PillowWriter\\nimport numpy as np\\nfrom torchvision import datasets, transforms\\nfrom torch.utils.data import DataLoader, random_split\\nimport torch\\nimport torchvision.models as models\\nimport torchvision.transforms as transforms\\nfrom torchvision.datasets import MNIST\\nfrom torchvision.transforms import Grayscale\\n\\ndef space_to_depth(x, size=2):\\n    \"\"\"\\n    Downsacle method that use the depth dimension to\\n    downscale the spatial dimensions\\n    Args:\\n        x (torch.Tensor): a tensor to downscale\\n        size (float): the scaling factor\\n\\n    Returns:\\n        (torch.Tensor): new spatial downscale tensor\\n    \"\"\"\\n    b, c, h, w = x.shape\\n    out_h = h // size\\n    out_w = w // size\\n    out_c = c * (size * size)\\n    x = x.reshape((-1, c, out_h, size, out_w, size))\\n    x = x.permute((0, 1, 3, 5, 2, 4))\\n    x = x.reshape((-1, out_c, out_h, out_w))\\n    return x\\n\\n\\nclass SpaceToDepth(nn.Module):\\n  def __init__(self, size):\\n    super().__init__()\\n    self.size = size\\n\\n  def forward(self, x):\\n    return space_to_depth(x, self.size)\\n\\n\\nclass Residual(nn.Module):\\n  \"\"\"\\n  Apply residual connection using an input function\\n  Args:\\n    func (function): a function to apply over the input\\n  \"\"\"\\n  def __init__(self, func):\\n    super().__init__()\\n    self.func = func\\n\\n  def forward(self, x, *args, **kwargs):\\n    return x + self.func(x, *args, **kwargs)\\n\\ndef upsample(in_channels, out_channels=None):\\n  out_channels = in_channels if out_channels is None else out_channels\\n  seq = nn.Sequential(\\n      nn.Upsample(scale_factor=2, mode=\\'nearest\\'),\\n      nn.Conv2d(in_channels, out_channels, 3, padding=1)\\n  )\\n  return seq\\n\\ndef downsample(in_channels, out_channels=None):\\n  out_channels = in_channels if out_channels is None else out_channels\\n  seq = nn.Sequential(\\n      SpaceToDepth(2),\\n      nn.Conv2d(4 * in_channels, out_channels, 1)\\n  )\\n  return seq\\n\\n\\n\\nclass SinusodialPositionEmbedding(nn.Module):\\n  def __init__(self, embedding_dim):\\n    super().__init__()\\n    self.embedding_dim = embedding_dim\\n\\n  def forward(self, time_steps):\\n    positions = torch.unsqueeze(time_steps, 1)\\n    half_dim = self.embedding_dim // 2\\n    embeddings = torch.zeros((time_steps.shape[0], self.embedding_dim), device=time_steps.device)\\n    denominators = 10_000 ** (2 * torch.arange(self.embedding_dim // 2, device=time_steps.device) / self.embedding_dim)\\n    embeddings[:, 0::2] = torch.sin(positions/denominators)\\n    embeddings[:, 1::2] = torch.cos(positions/denominators)\\n    return embeddings\\n\\nclass WeightStandardizedConv2d(nn.Conv2d):\\n  \"\"\"\\n  https://arxiv.org/abs/1903.10520\\n  \"\"\"\\n\\n  def forward(self, x):\\n    eps = 1e-5 if x.dtype == torch.float32 else 1e-3\\n\\n    weight = self.weight\\n    mean = weight.mean(dim=[1,2,3], keepdim=True)\\n    variance = weight.var(dim=[1,2,3], keepdim=True, correction=0)\\n    normalized_weight = (weight - mean) / torch.sqrt(variance)\\n\\n    return F.conv2d(\\n        x,\\n        normalized_weight,\\n        self.bias,\\n        self.stride,\\n        self.padding,\\n        self.dilation,\\n        self.groups\\n    )\\n\\n\\nclass Block(nn.Module):\\n  def __init__(self, in_channels, out_channels, groups=8):\\n    super().__init__()\\n    self.proj = WeightStandardizedConv2d(in_channels, out_channels, 3, padding=1)\\n    self.norm = nn.GroupNorm(groups, out_channels)\\n    self.act = nn.SiLU()\\n\\n  def forward(self, x, scale_shift=None):\\n    x = self.proj(x)\\n    x = self.norm(x)\\n\\n    if scale_shift is not None:\\n      scale, shift = scale_shift\\n      x = x * (scale + 1) + shift\\n\\n    x = self.act(x)\\n    return x\\n\\nclass ResnetBlock(nn.Module):\\n  def __init__(self, in_channels, out_channels, time_emb_dim=None, groups=8):\\n    super().__init__()\\n    if time_emb_dim is not None:\\n      self.mlp = nn.Sequential(\\n          nn.SiLU(),\\n          nn.Linear(time_emb_dim, 2 * out_channels)\\n      )\\n    else:\\n      self.mlp = None\\n\\n    self.block1 = Block(in_channels, out_channels, groups)\\n    self.block2 = Block(out_channels, out_channels, groups)\\n    if in_channels == out_channels:\\n      self.res_conv = nn.Identity()\\n    else:\\n      self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\\n\\n  def forward(self, x, time_emb=None):\\n    scale_shift = None\\n    if self.mlp is not None and time_emb is not None:\\n      time_emb = self.mlp(time_emb)\\n      time_emb = time_emb.view(*time_emb.shape, 1, 1)\\n      scale_shift = time_emb.chunk(2, dim=1) ########\\n\\n    h = self.block1(x, scale_shift=scale_shift)\\n    h = self.block2(h)\\n    return h + self.res_conv(x)\\n\\nclass Attention(nn.Module):\\n  def __init__(self, in_channels, num_heads=4, dim_head=32):\\n    super().__init__()\\n    self.num_heads = num_heads\\n    self.dim_head = dim_head\\n    self.scale_factor = 1 / (dim_head) ** 0.5\\n    self.hidden_dim = num_heads * dim_head\\n    self.input_to_qkv = nn.Conv2d(in_channels, 3 * self.hidden_dim, 1, bias=False)\\n    self.to_output = nn.Conv2d(self.hidden_dim, in_channels, 1)\\n\\n  def forward(self, x):\\n    b, c, h, w = x.shape\\n    qkv = self.input_to_qkv(x)\\n    q, k, v = map(lambda t: t.view(b, self.num_heads, self.dim_head, h * w), qkv.chunk(3, dim=1))\\n    q = q * self.scale_factor\\n    # dot product between the columns of q and k\\n    sim = torch.einsum(\"b h c i, b h c j -> b h i j\", q, k)\\n    sim = sim - sim.amax(dim=-1, keepdim=True).detach()\\n    attention = sim.softmax(dim=-1)\\n\\n    # dot product between the rows to get the wighted values as columns\\n    output = torch.einsum(\"b h i j, b h c j -> b h i c\", attention, v)\\n    output = output.permute(0, 1, 3, 2).reshape((b, self.hidden_dim, h, w))\\n    return self.to_output(output)\\n\\n\\nclass LinearAttention(nn.Module):\\n  def __init__(self, in_channels, num_heads=4, dim_head=32):\\n    super().__init__()\\n    self.num_heads = num_heads\\n    self.dim_head = dim_head\\n    self.scale_factor = 1 / (dim_head) ** 0.5\\n    self.hidden_dim = num_heads * dim_head\\n    self.input_to_qkv = nn.Conv2d(in_channels, 3 * self.hidden_dim, 1, bias=False)\\n    self.to_output = nn.Sequential(\\n        nn.Conv2d(self.hidden_dim, in_channels, 1),\\n        nn.GroupNorm(1, in_channels)\\n    )\\n\\n  def forward(self, x):\\n    b, c, h, w = x.shape\\n    qkv = self.input_to_qkv(x)\\n    q, k, v = map(lambda t: t.view(b, self.num_heads, self.dim_head, h * w), qkv.chunk(3, dim=1))\\n\\n    q = q.softmax(dim=-2)\\n    k = k.softmax(dim=-1)\\n\\n    q = q * self.scale_factor\\n    context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\\n    output = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\\n    output = output.view((b, self.hidden_dim, h, w))\\n    return self.to_output(output)\\n\\nclass PreGroupNorm(nn.Module):\\n  def __init__(self, dim , func, groups=1):\\n    super().__init__()\\n    self.func = func\\n    self.group_norm = nn.GroupNorm(groups, dim)\\n\\n  def forward(self, x):\\n    x = self.group_norm(x)\\n    x = self.func(x)\\n    return x'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I0O6gWd7OJrY"
      },
      "outputs": [],
      "source": [
        "# lybrires\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import HTML\n",
        "from typing import Dict, Tuple\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhoKX17W1Cui",
        "outputId": "8051398a-5749-4dce-ce85-32df55d27b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 43191146.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 2957053.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 11471822.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2837010.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 35.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# MNIST Dataset transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match ResNet18 input size\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split train dataset into train and validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load pre-trained ResNet18 model for embeddings\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "pretrained_model = nn.Sequential(*list(pretrained_model.children())[:-1])  # Remove last classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a2Xczv2C2n5x"
      },
      "outputs": [],
      "source": [
        "# this code saves embadigns for MNIST data set\n",
        "labels = []\n",
        "for images, targets in train_loader:\n",
        "    with torch.no_grad():\n",
        "        features = pretrained_model(images)\n",
        "        labels.append(features.view(features.size(0), -1))  # Flatten the embeddings\n",
        "\n",
        "# Concatenate embeddings and convert to tensor\n",
        "labels = torch.cat(labels, dim=0)\n",
        "\n",
        "# Save the labels (embeddings) to a file\n",
        "torch.save(labels, 'mnist_labels.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7oyxUGpz1Rmf",
        "outputId": "96c5b60f-06fd-406a-e79f-2646b52fe37c"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (3, 224, 224) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e160e11eb1a5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert tensor to numpy array and remove the channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    709\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 710\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    711\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 224, 224) for image data"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 700x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize some sample images from the training set\n",
        "plt.figure(figsize=(7, 7))\n",
        "for i in range(25):\n",
        "    image, label = train_dataset[i]\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(image.squeeze().numpy(), cmap='gray')  # Convert tensor to numpy array and remove the channel dimension\n",
        "    plt.title(\"Label: \" + str(label))\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WS2QKBBNwah-"
      },
      "outputs": [],
      "source": [
        "from diffusion_model_classes import SpaceToDepth, SinusodialPositionEmbedding, ResnetBlock, Residual, PreGroupNorm,\n",
        "LinearAttention, downsample, Attention, upsample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2ZBXJP1TAh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0tsmvK5tkQzH"
      },
      "outputs": [],
      "source": [
        "class DiffusionUnet(nn.Module):\n",
        "  def __init__(self, dim, init_dim=None, output_dim=None, dim_mults=(1, 2, 4, 8), channels=3, resnet_block_groups=4):\n",
        "    super().__init__()\n",
        "\n",
        "    self.channels = channels\n",
        "    init_dim = init_dim if init_dim is not None else dim\n",
        "    self.init_conv = nn.Conv2d(self.channels, init_dim, 1)\n",
        "    dims = [init_dim] + [m * dim for m in dim_mults]\n",
        "    input_output_dims = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "    time_dim = 4 * dim  # time embedding\n",
        "\n",
        "    self.time_mlp = nn.Sequential(\n",
        "        SinusodialPositionEmbedding(dim),\n",
        "        nn.Linear(dim, time_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(time_dim, time_dim)\n",
        "    )\n",
        "\n",
        "    # down layers\n",
        "    self.down_layers = nn.ModuleList([])\n",
        "    for ii, (dim_in, dim_out) in enumerate(input_output_dims, 1):\n",
        "      is_last = ii == len(input_output_dims)\n",
        "      self.down_layers.append(\n",
        "          nn.ModuleList(\n",
        "              [\n",
        "                  ResnetBlock(dim_in, dim_in, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                  ResnetBlock(dim_in, dim_in, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                  Residual(PreGroupNorm(dim_in, LinearAttention(dim_in))),\n",
        "                  downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "\n",
        "      # middle layers\n",
        "      mid_dim = dims[-1]\n",
        "      self.mid_block1 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "      self.mid_attention = Residual(PreGroupNorm(mid_dim, Attention(mid_dim)))\n",
        "      self.mid_block2 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "\n",
        "      # up layers\n",
        "      self.up_layers = nn.ModuleList([])\n",
        "      for ii, (dim_in, dim_out) in enumerate(reversed(input_output_dims), 1):\n",
        "        is_last = ii == len(input_output_dims)\n",
        "        self.up_layers.append(\n",
        "            nn.ModuleList(\n",
        "                [\n",
        "                    ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                    ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                    Residual(PreGroupNorm(dim_out, LinearAttention(dim_out))),\n",
        "                    upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.output_dim = output_dim if output_dim is not None else channels\n",
        "        self.final_res_block = ResnetBlock(2 * dim, dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "        self.final_conv = nn.Conv2d(dim, self.output_dim, 1)\n",
        "\n",
        "  def forward(self, x, time):\n",
        "    x = self.init_conv(x)\n",
        "    init_result = x.clone()\n",
        "    t = self.time_mlp(time)\n",
        "    h = []\n",
        "\n",
        "    for block1, block2, attention, downsample_block in self.down_layers:\n",
        "      x = block1(x, t)\n",
        "      h.append(x)\n",
        "\n",
        "      x = block2(x, t)\n",
        "      x = attention(x)\n",
        "\n",
        "      h.append(x)\n",
        "\n",
        "      x = downsample_block(x)\n",
        "\n",
        "    x = self.mid_block1(x, t)\n",
        "    x = self.mid_attention(x)\n",
        "    x = self.mid_block2(x ,t)\n",
        "\n",
        "    for block1, block2, attention, upsample_block in self.up_layers:\n",
        "      x = torch.cat((x , h.pop()), dim=1)\n",
        "      x = block1(x, t)\n",
        "\n",
        "      x = torch.cat((x, h.pop()), dim=1)\n",
        "      x = block2(x, t)\n",
        "\n",
        "      x = attention(x)\n",
        "\n",
        "      x = upsample_block(x)\n",
        "\n",
        "    x = torch.cat((x, init_result), dim=1)\n",
        "    x = self.final_res_block(x, t)\n",
        "    x = self.final_conv(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbp79sDWRkkj"
      },
      "source": [
        "**2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4xlL3mdOZu4"
      },
      "outputs": [],
      "source": [
        "# diffusion hyperparameters\n",
        "timesteps = 500\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.02\n",
        "\n",
        "# network hyperparameters\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
        "n_feat = 64 # 64 hidden dimension feature\n",
        "n_cfeat = 5 # context vector is of size 5\n",
        "height = 28 # 16x16 image\n",
        "save_dir = './weights/'\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 100\n",
        "n_epoch = 32\n",
        "lrate=1e-3\n",
        "\n",
        "# construct DDPM noise schedule\n",
        "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
        "a_t = 1 - b_t\n",
        "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
        "ab_t[0] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lajvp0RmWj"
      },
      "source": [
        "**3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VN8fWmsNQI2N"
      },
      "outputs": [],
      "source": [
        "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
        "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KHH9KMdnm6wY"
      },
      "outputs": [],
      "source": [
        "def perturb_input(x, t, noise):\n",
        "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9u0n_MenBuw"
      },
      "source": [
        "**4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TwTQkzwJnAcW"
      },
      "outputs": [],
      "source": [
        "# training with context code\n",
        "# set into train mode\n",
        "nn_model.train()\n",
        "\n",
        "for ep in range(n_epoch):\n",
        "    print(f'epoch {ep}')\n",
        "\n",
        "    # linearly decay learning rate\n",
        "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
        "\n",
        "    pbar = tqdm(dataloader, mininterval=2 )\n",
        "    for x, c in pbar:   # x: images  c: context\n",
        "        optim.zero_grad()\n",
        "        x = x.to(device)\n",
        "        c = c.to(x)\n",
        "\n",
        "        # randomly mask out c\n",
        "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
        "        c = c * context_mask.unsqueeze(-1)\n",
        "\n",
        "        # perturb data\n",
        "        noise = torch.randn_like(x)\n",
        "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
        "        x_pert = perturb_input(x, t, noise)\n",
        "\n",
        "        # use network to recover noise\n",
        "        pred_noise = nn_model(x_pert, t / timesteps, c=c)\n",
        "\n",
        "        # loss is mean squared error between the predicted and true noise\n",
        "        loss = F.mse_loss(pred_noise, noise)\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "      # save model periodically\n",
        "    if ep%4==0 or ep == int(n_epoch-1):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        torch.save(nn_model.state_dict(), save_dir + f\"context_model_{ep}.pth\")\n",
        "        print('saved model at ' + save_dir + f\"context_model_{ep}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xe9HZOQAoPMv"
      },
      "outputs": [],
      "source": [
        "# load in pretrain model weights and set to eval mode\n",
        "# u can also not to run\n",
        "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_trained.pth\", map_location=device))\n",
        "nn_model.eval()\n",
        "print(\"Loaded in Context Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rvXSI_VRs19"
      },
      "source": [
        "**5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3mb7NPPdQPva"
      },
      "outputs": [],
      "source": [
        "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
        "def denoise_add_noise(x, t, pred_noise, z=None):\n",
        "    if z is None:\n",
        "        z = torch.randn_like(x)\n",
        "    noise = b_t.sqrt()[t] * z\n",
        "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
        "    return mean + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD6FwjAjRvQr"
      },
      "source": [
        "**6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bsl4ivVSw8xr"
      },
      "outputs": [],
      "source": [
        "# define sampling function for DDIM\n",
        "# removes the noise using ddim\n",
        "def denoise_ddim(x, t, t_prev, pred_noise):\n",
        "    ab = ab_t[t]\n",
        "    ab_prev = ab_t[t_prev]\n",
        "\n",
        "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
        "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
        "\n",
        "    return x0_pred + dir_xt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRDDr2s6R2mL"
      },
      "source": [
        "**7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HA-lPXs8Qzw9"
      },
      "outputs": [],
      "source": [
        "# fast sampling algorithm with context\n",
        "@torch.no_grad()\n",
        "def sample_ddim_context(n_sample, context, n=20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    step_size = timesteps // n\n",
        "    for i in range(timesteps, 0, -step_size):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
        "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
        "        intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO8pqtOER33R"
      },
      "source": [
        "**8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PEQm8xWSQ0B-"
      },
      "outputs": [],
      "source": [
        "# visualize samples\n",
        "plt.clf()\n",
        "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
        "samples, intermediate = sample_ddim_context(32, ctx)\n",
        "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
        "HTML(animation_ddpm_context.to_jshtml())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCcJSPXGn5rFlzazvJRu+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}